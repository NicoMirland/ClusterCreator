---
- name: Ensure template was created correctly
  hosts: all
  tags:
    - ensure_space
  tasks:
    - name: Find all matching firstboot log files
      find:
        paths: "/var/log"
        patterns: "template-firstboot-*.log"
      register: log_files
    - name: Check firstboot log files for failures to ensure all packages were installed correctly (excluding specific patterns)
      shell: |
        grep -iE 'Error|Fatal' {{ item.path }} | grep -vE 'mysql/error.log|errors = false|liberror|No error reported|Failed to open terminal.debconf'
      # ADD EXCLUSIONS HERE^
      register: grep_output
      failed_when: grep_output.rc == 0
      with_items: "{{ log_files.files }}"
      ignore_errors: true
      no_log: true
    - name: Warn and pause if 'Error' or 'Fatal' is found in any firstboot log files, excluding specific instances
      pause:
        prompt: "Warning: 'Error' or 'Fatal' strings found in firstboot log files. Template VM may have an issue with its firstboot scripts. Investigate /var/log/template-firstboot-*.log files and, if needed, recreate the template and the VM(s). You can add exclusions in the prepare-nodes playbook (~ line 26) to filter out false-positives. Press enter to ignore this warning or Ctrl+C to abort."
      when: grep_output.results | selectattr('rc', 'equalto', 0) | list | count > 0

- name: Prepare nodes
  hosts: all
  any_errors_fatal: true
  become: true  # This ensures all tasks are run with elevated privileges
  vars:
    cluster_config: "{{ lookup('file', 'tmp/{{ cluster_name }}/cluster_config.json') | from_json }}"
  tasks:

    - name: Add IPv6 forwarding line to sysctl.d file (if IPv6 is enabled)
      ansible.builtin.lineinfile:
        path: /etc/sysctl.d/k8s_sysctl.conf
        line: 'net.ipv6.conf.all.forwarding        = 1'
        state: present
      when: cluster_config.networking.ipv6.enabled and cluster_config.networking.ipv6.dual_stack
    - name: Reload sysctl to apply changes (if IPv6 is enabled)
      ansible.builtin.command: sysctl --system
      when: cluster_config.networking.ipv6.enabled and cluster_config.networking.ipv6.dual_stack

    - name: Configure kubelet with the correct interface IP
      shell: |
        {% if cluster_config.networking.ipv6.enabled and cluster_config.networking.ipv6.dual_stack %}
        node_ip="{{ ipv4 }},{{ ipv6 }}"
        {% else %}
        node_ip="{{ ipv4 }}"
        {% endif %}

        kubelet_extra_args="--node-ip=${node_ip} --max-pods={{ cluster_config.max_pods_per_node }}"

        echo "KUBELET_EXTRA_ARGS=\"$kubelet_extra_args\"" | sudo tee /etc/default/kubelet
      args:
        executable: /bin/bash
      tags:
        - kubelet_node_ip

    # needed on recent Ubuntu versions. https://www.reddit.com/r/kubernetes/comments/1dv6z9d/ubuntu_2404_pod_termination_issue/
    - name: Check if /etc/apparmor.d/runc profile exists
      ansible.builtin.stat:
        path: /etc/apparmor.d/runc
      register: apparmor_runc_file_exists
    - name: Check if /etc/apparmor.d/runc profile is disabled
      ansible.builtin.stat:
        path: /etc/apparmor.d/disable/runc
      register: apparmor_runc_file_disabled
    - name: Disable Default AppArmor Profile for Runc (containerd has its own)
      ansible.builtin.command: sudo ln -s /etc/apparmor.d/runc /etc/apparmor.d/disable/
      ignore_errors: yes
      tags:
        - disable_apparmor
      when: apparmor_runc_file_exists.stat.exists and not apparmor_runc_file_disabled.stat.exists
    - name: Reload AppArmor
      ansible.builtin.command: sudo apparmor_parser -R /etc/apparmor.d/runc
      ignore_errors: yes
      tags:
        - disable_apparmor
      when: apparmor_runc_file_exists.stat.exists and not apparmor_runc_file_disabled.stat.exists

    - name: Set correct locale
      shell: |
        echo "LANG=en_US.UTF-8
        LANGUAGE=en_US" > /etc/default/locale
        localectl set-locale LANG=en_US.UTF-8
        touch /var/lib/cloud/instance/locale-check.skip
      args:
        executable: /bin/bash
      tags:
        - set_locale

    - name: Set helpful aliases
      shell: |
        echo "alias k='kubectl'" | tee -a {{ cluster_config.ssh.ssh_home }}/.bashrc
        echo "alias c='clear'" | tee -a {{ cluster_config.ssh.ssh_home }}/.bashrc
        echo "alias h='history'" | tee -a {{ cluster_config.ssh.ssh_home }}/.bashrc
      args:
        executable: /bin/bash
      tags:
        - set_aliases

    - name: Check if any scripts exist in /root directory
      stat:
        path: /root/*.sh
      register: scripts_exist

    - name: Move scripts from root directory to user directory (if any exist)
      shell: |
        cp /root/*.sh {{ cluster_config.ssh.ssh_home }}/
        chmod +x {{ cluster_config.ssh.ssh_home }}/*.sh
      args:
        executable: /bin/bash
      become: true
      when: scripts_exist.stat.exists
      tags:
        - copy_scripts

    - name: Add VIP and hostname to /etc/hosts
      ansible.builtin.lineinfile:
        path: /etc/hosts
        line: "{{ cluster_config.networking.kube_vip.vip }} {{ cluster_config.networking.kube_vip.vip_hostname }}"
        create: yes
      tags: etc_hosts
    - name: Add VIP and hostname to cloud templates (if exists)
      ansible.builtin.lineinfile:
        path: /etc/cloud/templates/hosts.debian.tmpl
        line: "{{ cluster_config.networking.kube_vip.vip }} {{ cluster_config.networking.kube_vip.vip_hostname }}"
        create: yes
      when: ansible_facts['os_family'] == "Debian"
      tags: etc_hosts
    - name: Update /etc/hosts with node entries
      ansible.builtin.lineinfile:
        path: /etc/hosts
        line: "{{ hostvars[item].ansible_host }} {{ item }}"
        state: present
      loop: "{{ groups['all'] }}"
      tags:
        - etc_hosts
    - name: Update /etc/hosts with node entries to cloud templates (if exists)
      ansible.builtin.lineinfile:
        path: /etc/cloud/templates/hosts.debian.tmpl
        line: "{{ hostvars[item].ansible_host }} {{ item }}"
        state: present
      loop: "{{ groups['all'] }}"
      when: ansible_facts['os_family'] == "Debian"
      tags:
        - etc_hosts

- name: Configure settings for nodes with nvidia gpus attached
  hosts: all
  become: true
  gather_facts: no
  tags:
    - nvidia
  tasks:
    - name: Check for NVIDIA GPU presence
      ansible.builtin.shell: lspci | grep -i nvidia
      register: nvidia_check
      failed_when: false

    - name: Set fact if NVIDIA GPU is present
      ansible.builtin.set_fact:
        has_nvidia_gpu: "{{ nvidia_check.rc == 0 }}"

    - name: End play on nodes without NVIDIA GPU
      ansible.builtin.meta: end_host
      when: not has_nvidia_gpu | default(false)

    - name: Ensure Nvidia GPU Drivers are present
      ansible.builtin.command: nvidia-smi
      register: nvidia_output

    - name: Display Nvidia GPU info
      ansible.builtin.debug:
        msg: "{{ nvidia_output.stdout }}"

    - name: Enable persistence mode for NVIDIA GPU
      ansible.builtin.command: sudo nvidia-smi -pm 1

    - name: Reset application clocks to default
      ansible.builtin.command: sudo nvidia-smi -rac

    - name: Gather service facts
      ansible.builtin.setup:
        filter: ansible_facts.services

    - name: Create systemd service for NVIDIA GPU power management
      ansible.builtin.copy:
        dest: /etc/systemd/system/nvidia-power-management.service
        content: |
          [Unit]
          Description=NVIDIA GPU Power Management
          After=network.target

          [Service]
          Type=oneshot
          ExecStart=/usr/bin/nvidia-smi -pm 1
          ExecStart=/usr/bin/nvidia-smi -rac
          RemainAfterExit=true

          [Install]
          WantedBy=multi-user.target
      when: not ansible_facts.services.get('nvidia-power-management')

    - name: Reload systemd daemon to recognize new service
      ansible.builtin.command: sudo systemctl daemon-reload
      when: not ansible_facts.services.get('nvidia-power-management')

    - name: Enable and start NVIDIA GPU power management service
      ansible.builtin.systemd:
        name: nvidia-power-management
        enabled: true
        state: started

    - name: Check if containerd is configured with NVIDIA runtime
      ansible.builtin.command: cat /etc/containerd/config.toml | grep "nvidia"
      register: containerd_config
      ignore_errors: yes
      changed_when: false

    - name: Configure containerd to use Nvidia runtime
      ansible.builtin.shell: |
        sudo nvidia-ctk runtime configure --runtime=containerd
      when: containerd_config.rc != 0
      notify:
        - Restart containerd

  handlers:
    - name: Restart containerd
      ansible.builtin.systemd:
        name: containerd
        state: restarted
